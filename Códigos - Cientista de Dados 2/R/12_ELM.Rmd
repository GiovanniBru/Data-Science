Extreme Learning Machine: 

```{r}
#install.packages("elmNNRcpp")
#install.packages("mltools")
#install.packages("data.table")
#install.packages("caret")
library(elmNNRcpp)  # Pacote de ELM
library(mltools)
library(data.table)
library(caret)
```

Particionando em treino e teste: 
```{r}
set.seed(1234) # Mesma semente do outro código 
particao = createDataPartition(1:dim(iris)[1],p=.7)
iristreino = iris[particao$Resample1,]
iristeste = iris[- particao$Resample1,]
```

Tratando dados de treino, separamos X e Y: 
```{r}
x = iristreino[,-5]
y = iristreino[,5]
# Separamos as variáveis dependentes (X) da variável independente (Y)
```

Dimensionamento de Característica das variáveis independentes e One-Hot na independente: 
```{r}
x = scale(x)
#codificação de categoria para a classe
y = one_hot(one_hot(as.data.table(y)))
```

Precisamos transformar todas as variáveis em matriz, pois o método que faz o treinamento do ELM exige esse formato: 
```{r}
x = as.matrix(x)
y = as.matrix(y)
```

Criando modelo: 
```{r}
# nhid = neuronios ocultos
#modelo = elm_train(x, y, nhid = 1000, actfun = 'relu', init_weights = "uniform_negative", 
#                   bias = TRUE, verbose = TRUE)
# init_weights = distribuição que iremos utilizar pra definir aleatorimente os pesos das sinapses da camada de entrada com a camada oculta 
# Verbose TRUE gera uma saída 

modelo = elm_train(x, y, nhid = 100, actfun = 'sig', init_weights = "normal_gaussian", 
                   bias = TRUE, verbose = TRUE)
```
Time to complete : 0.212877 secs 
O modelo foi muito rápido!

Preparando dados de teste:
```{r}
# Salvamos a classe para no final comparar com a previsão
iristesteSpecies = iristeste$Species
# dimensionamento de caracteristicas
iristeste = scale(iristeste[,-5])
# transformamos em matrix
iristeste = as.matrix(iristeste)
```

Previsão:
```{r}
resultado = elm_predict(modelo,iristeste, normalize = TRUE)
  # Normalize=TRUE para casos de classificação 
```

Transformando em DataFrame: 
```{r}
resultado = as.data.frame(resultado)
resultado
```
Previsão também saiu como probabilidade

Dando nome as colunas:
```{r}
names(resultado)[1] = 'setosa'
names(resultado)[2] = 'versicolor'
names(resultado)[3] = 'virginica'
resultado
```

Criamos uma coluna classe que recebe o maior valor da linha: 
```{r}
resultado$class = colnames(resultado[,1:3])[max.col(resultado[,1:3], ties.method = 'first')]
resultado
```

Calculando o resultado: 
```{r}
confusao = table(resultado$class,iristesteSpecies)
confusao
sum(diag(confusao) * 100 / sum(confusao))
```

Tivemos uma acurácia de 90% com: actfun = 'sig', init_weights = "normal_gaussian"
E uma acurácia de 84% com: actfun = 'relu', init_weights = "uniform_negative"

Com a acurácia de 90% podemos provar que o ELM tem seu valor, apesar de não ter o mesmo valor da Multilayer